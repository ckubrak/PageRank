
\par Las relaciones entre p\'aginas forman un grafo y resulta conveniente almacenarlos como una
matriz de uno y ceros, done el un uno en la poscici\'on $ij$ representa que la p\'agina $j$ apunta a la $i$.
A\'un as\'i esta implementaci\'ion tiene un problema: debe almacenar $n^2$ elementos, donde solo importan
$m$ (la cantidad de unos). En la pr\'actica, $m \ll n^2$ (ya que no todas las p\'aginas se relacionan
con todos). Es por esto que decidimos utilizar matrices \textit{ralas}: un tipo de matriz donde solo
importa donde hay unos, es decir, qu\'e p\'agina apunta a cu\'al.
\par Como primer acercamiento, evaluamos utilizar dos conocidos conocidos m\'etodos para el almacenamiento
de matrices ralas: CSR (Compressed Sparse Row) y CSC (Compressed Sparse Column). En el primer caso
nos encotramos con el problema de la dificultad de acceder a las colmunas, mientras que en el segundo,
las filas. consideramos de vital importancia poder acceder tanto a filas como columnas en un tiempo
razonable para operaciones tales como la multiplicación o la Eliminación Gausseana.
\par Luego de evaluar los requerimientos, tanto de complejidad como de espacio utilizado, optamos
por implementar una estructura h\'ibrida entre un DOK (Dictionary Of Keys) y una lista de listas. 
Utilizamos una estructura que consiste en un diccionario de diccionarios: en el primero almacenamos
todas las filas y en el segundo sus elementos.
\par Respecto a la implementaci\'on, nos encontramos frente a la decisión de utilizar un diccionario ordenado
implementado sobre una estructura autobalanceada (\verb|std::map|) o un diccionario desordenado
implementado sobre una tabla de hash (\verb|std::unordered_map|). Si bien este \'ulitmo permite
el acceso en $O(1)$ en promedio frente al acceso en $O(\log n)$ del diccionario ordenado, 
no resulta f\'acil iterar eficientemete por lo que terminamos decidi\'endonos por la versi\'on autobalanceada.
\par Nuestro objectivo es resolver el listem a \ref{eq:Axx} o equivalentemente el sistema \ref{ipwd}, para lo cual 
resulta pr\'actico y eficiente utilizar Eliminación Gausseana:

\begin{codebox}
\Procname{$\proc{Eliminación Gausseana}(A)$}
\li \For $k \gets 1$ \To $n-1$
    \Do
\li     \For $i \gets k+1$ \To $n$
            \Do
\li         \For $j \gets k+1$ \To $n$
                \Do
\li                 $a_{ij} \gets a_{ij} - a_{ik}a_{kj}$
                \End
            \End
        \End
\end{codebox}

\par Como la matriz que utlizamos para almacenar la informci\'on es rala (no todos los elementos se encuentran definidos) por lo que 
adem\'as hay que asegurarse que los elementos est\'en definidos antes de operar con ellos. M\'as a\'un, dado que la resoluci\'on
del sistema no es m\'as que una aproximaci\'on, es necesario definir un $\varepsilon > 0$ para determinar qu\'e tan bien queremos que 
aproxime a la soluci\'on real; si bien un $\varepsilon$ m\'as chico producir\'ia una mejor aproximaci\'on, tambi\'en ralentiza la
ejecuci\'on del algoritmo. Es por esto, que junto con la esparsidad ($\delta = 1 - \frac{m}{n^2}$), decidimos hacer un an\'alisis
sobre el taman\~no del $\varepsilon$ por lo que el algoritmo de resoluci\'on queda:

\begin{codebox}
\Procname{$\proc{Eliminación Gausseana Rala}(A)$}
\li \For $k \gets 1$ \To $n-1$
    \Do
\li     \For $i \gets k+1$ \To $n$
            \Do
\li         \For $j \gets k+1$ \To $n$
                \Do
\li                \If $a_{ik}.definido()$ and $a_{kj}.definido()$
                        \Then
\li                        $mult \gets a_{ik}/a_{kk}$
\li                     \If $a_{ij}.definido()$
                            \Then
\li                          $a_{ij} \gets a_{ij} - mult*a_{kj}$
\li                     \Else
\li                          $a_{ij} \gets - mult*a_{kj}$
                \End
            \End
        \End
\end{codebox}
Observar que en ning\'un momento verificamos que $a_{kk}$ est\'e definido ya que la matriz permite hacer Eliminación Gausseana sin necesidad
de pivoteo. \textbf{DECIR POR QUE}
Luego para determinar las soluciones del sistema\cite{burden}:
\[
    x_n = \frac{a_{n,n-1}}{a_{nn}}
    \]
    \[
    x_i = \frac {a_{i,n+1} - \sum_{j=i+1}^{n} a_{ij}x} {a_{ii}}
    \]

Por \'ulitmo, normalizamos el vector $x$ de manera que $\sum^{n}_{i=1} |x_i| = 1$
\subsection*{Experimentaci\'on}
\par
En cuanto a la experimentaci\'on, pensamos en probar nuestra implementaci\'on de dos formas diferentes: de manera cualitativa y cuantitiativa.
Para realizar el an\'alisis cualitativo, generamos matrices aleatorias para luego variar los distintos parametros tales como probabilidad, 
tamaño de la matriz, cantidad de links y epsilon, para poder luego evaluar su incidencia. Para el an\'alisis cualitativo, construimos casos particulares
que nos resultaron interesantes analizar por diversos motivos.

\par 

Con respepcto al an\'alisis cuantitiativo, varianmos los tres par\'ametros que considerados que podr\'ian ser interesantes de analizar:
el tama\~no de la matriz $n$, la cantidad de links $m$, la probabilidad $p$ y el $\varepsilon$.
\newline
En el primero de estos tests, creamos matrices de entrada aleatorias con tamaño $n$, probabilidad $p$ y $\varepsilon$ fijos, mientras variaban la cantidad de links $m$
de la matriz generada, es decir cambiando qu\'e tan rala es. A partir de esto, buscamos encontrar una relaci\'on entre la \textit{sparsity} y el tiempo.\newline
En este test nuestra expectativa fue encontrarnos con un incremento en el tiempo a medida 
que la \textit{sparsity} de nuestra matriz de entrada era menor, considerando que la
implementaci\'on fue diseñada para funcionar de manera eficiente en matrices de este tipo.

\par
Como segundo test, buscamos evaluar incidencia del parametro $p$ en el tiempo de ejecuci\'on en una matriz de tamaño $m$, cantidad de links $n$ 
y $\varepsilon$ fijos.\newline
Al ver la ecuaci\'on \ref{ipwd} observamos que a media que el parametro $p$ decrece, tambi\'en lo hace el resultado de $pWD$ y, 
por la forma en que implementamos la igualdad, tener elementos m\'as pequeños en una matriz implica que m\'as elementos de la misma podr\'ian 
ser considerados ceros (dependiendo del $\varepsilon$ que estemos utilizando).\\
Esto nos hace pensar que resolver la ecuaci\'on anterior en con un $p$ relativamente pequeño  requerir\'ia menos tiempo.
Por \'ultimo, no quisimos pasar por alto otro aspecto que consideramos que consideramos que podr\'ia tener cierta relevancia en este test, el n\'umero de condici\'on. 
Como est\'a señalado en el ap\'endice, vimos que a menor $p$, mejor condicionada est\'a la matriz, entonces a medida que aumenta $p$, 
aumentar\'a tambi\'en la cota superior que tenemos para el n\'umero de condici\'on, con lo que nuestro calculo podr\'ia volverse menos estable, 
en el sentido de a pesar de no conocer el error, nos exponemos a que sea m\'as grande.
Si bien esto nos abre el camino a nuevas experimentaciones, entendemos que el eje de este trabajo no es este, es por esto que simplemente lo mencionamos como un posible tema de inter\'es para una pr\'oxima investigaci\'on.

\par
Como \'ultima pruba cualitativa, mantuvimos fijos el tama\~no de la matriz $n$, la cantidad de links $m$, la probabilidad $p$ y variamos la precisi\'on del $\varepsilon$.
A la hora de realizar este test, estimamos que la variaci\'on del $\varepsilon$ no afectar\'a de manera significativa el tiempo de ejecuci\'on.